{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content</h3>\n\n* [1. Libraries](#1)\n* [2. Reading the Data](#2)\n    - [1.1 An Overview from the Data](#2.1)\n* [3. Exploratory Data Analysis](#3) \n    - [3.1 Total Orders on E-Commerce](#3.1)\n    - [3.2 E-Commerce Around Brazil](#3.2)\n    - [3.3 E-Commerce Impact on Economy](#3.3)\n    - [3.4 Payment Type Analysis](#3.4)\n* [4. Natural Language Processing](#4)\n    - [4.1 Data Understanding](#4.1)\n    - [4.2 Regular Expressions](#4.2)\n        - [4.2.1 Break Line and Carriage Return](#4.2.1)\n        - [4.2.2 Sites and Hiperlinks](#4.2.2)\n        - [4.2.3 Dates](#4.2.3)\n        - [4.2.4 Money](#4.2.4)\n        - [4.2.5 Numbers](#4.2.5)\n        - [4.2.6 Negation](#4.2.6)\n        - [4.2.7 Special Characteres](#4.2.7)\n        - [4.2.8 Additional Whitespaces](#4.2.8)\n    - [4.3 Stopwords](#4.3)\n    - [4.4 Stemming](#4.4)\n    - [4.5 Feature Extraction](#4.5)\n        - [4.5.1 CountVectorizer](#4.5.1)\n        - [4.5.2 TF-IDF](#4.5.2)\n    - [4.6 Labeling Data](#4.6)\n    - [4.7 Pipeline](#4.7)\n* [5. Sentiment Classification](#5)\n* [6. Final Implementation](#6)\n* [7. Conclusion](#7)\n* [8. Complete Script](#8)","metadata":{"toc":true}},{"cell_type":"markdown","source":"The objective of this notebook is to propose an analytical view of e-commerce relationship in Brazil. For this we will first go trough an exploratory data analysis using graphical tools to create self explanatory plots for better understanding what is behind braziian online purchasing. Finally we will look at customers reviews and implement **_Sentimental Analysis_** to make a text classification using **_Natural Language Process_** tools.\n\nWe will go trough a extensive journey for understanding the data and plotting some useful charts to clarify the concepts and get insights from data and, at the end, we will go trough a step-by-step code on text preparating and sentiment classification using the reviews left from customer on online platforms. I hope you enjoy this notebook!\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Libraries</b></font>","metadata":{}},{"cell_type":"code","source":"pip install folium --upgrade","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T18:09:48.995196Z","iopub.execute_input":"2021-11-24T18:09:48.995683Z","iopub.status.idle":"2021-11-24T18:09:55.724808Z","shell.execute_reply.started":"2021-11-24T18:09:48.995633Z","shell.execute_reply":"2021-11-24T18:09:55.723731Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Standard libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib.gridspec import GridSpec\npd.set_option('display.max_columns', 100)\nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport json\nimport requests\nimport folium\nfrom folium.plugins import FastMarkerCluster, Fullscreen, MiniMap, HeatMap, HeatMapWithTime, LocateControl\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom PIL import Image\n\n# Utilities\nfrom viz_utils import *\nfrom custom_transformers import *\nfrom ml_utils import *\n\n# DataPrep\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import RSLPStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nimport joblib\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport lightgbm as lgb","metadata":{"ExecuteTime":{"start_time":"2020-07-05T23:57:02.940624Z","end_time":"2020-07-05T23:57:06.62821Z"},"execution":{"iopub.status.busy":"2021-11-24T18:09:55.726748Z","iopub.execute_input":"2021-11-24T18:09:55.727032Z","iopub.status.idle":"2021-11-24T18:10:02.158334Z","shell.execute_reply.started":"2021-11-24T18:09:55.726983Z","shell.execute_reply":"2021-11-24T18:10:02.157369Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>2. Reading the Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"For this task we have differente data sources, each one describing a specific topic related to e-commerce sales. The files are:\n\n    olist_customers_dataset.csv\n    olist_geolocation_dataset.csv\n    olist_orders_dataset.csv\n    olist_order_items_dataset.csv\n    olist_order_payments_dataset.csv\n    olist_order_reviews_dataset.csv\n    olist_products_dataset.csv\n    olist_sellers_dataset.csv\n    product_category_name_translation.csv\n    \nThe relationship between these files are described on the documentation. So let's read the datasets and make an initial analysis with all of them. This step will help us a lot to take right decisions in a future exploratory data analysis.","metadata":{}},{"cell_type":"code","source":"# Reading all the files\nraw_path = '../input/brazilian-ecommerce/'\nolist_customer = pd.read_csv(raw_path + 'olist_customers_dataset.csv')\nolist_geolocation = pd.read_csv(raw_path + 'olist_geolocation_dataset.csv')\nolist_orders = pd.read_csv(raw_path + 'olist_orders_dataset.csv')\nolist_order_items = pd.read_csv(raw_path + 'olist_order_items_dataset.csv')\nolist_order_payments = pd.read_csv(raw_path + 'olist_order_payments_dataset.csv')\nolist_order_reviews = pd.read_csv(raw_path + 'olist_order_reviews_dataset.csv')\nolist_products = pd.read_csv(raw_path + 'olist_products_dataset.csv')\nolist_sellers = pd.read_csv(raw_path + 'olist_sellers_dataset.csv')","metadata":{"ExecuteTime":{"start_time":"2020-07-05T23:57:06.632071Z","end_time":"2020-07-05T23:57:12.887408Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:02.160053Z","iopub.execute_input":"2021-11-24T18:10:02.160359Z","iopub.status.idle":"2021-11-24T18:10:05.819216Z","shell.execute_reply.started":"2021-11-24T18:10:02.160309Z","shell.execute_reply":"2021-11-24T18:10:05.818247Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>2.1 An Overview from the Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Before creating a unique dataset with all useful information, let's look at the shape of each dataset, so we can be more assertive on how to use joining statements. ","metadata":{}},{"cell_type":"code","source":"# Collections for each dataset\ndatasets = [olist_customer, olist_geolocation, olist_orders, olist_order_items, olist_order_payments,\n            olist_order_reviews, olist_products, olist_sellers]\nnames = ['olist_customer', 'olist_geolocation', 'olist_orders', 'olist_order_items', 'olist_order_payments',\n         'olist_order_reviews', 'olist_products', 'olist_sellers']\n\n# Creating a DataFrame with useful information about all datasets\ndata_info = pd.DataFrame({})\ndata_info['dataset'] = names\ndata_info['n_rows'] = [df.shape[0] for df in datasets]\ndata_info['n_cols'] = [df.shape[1] for df in datasets]\ndata_info['null_amount'] = [df.isnull().sum().sum() for df in datasets]\ndata_info['qty_null_columns'] = [len([col for col, null in df.isnull().sum().items() if null > 0]) for df in datasets]\ndata_info['null_columns'] = [', '.join([col for col, null in df.isnull().sum().items() if null > 0]) for df in datasets]\n\ndata_info.style.background_gradient()","metadata":{"ExecuteTime":{"start_time":"2020-07-05T23:57:12.890456Z","end_time":"2020-07-05T23:57:15.548767Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:05.820619Z","iopub.execute_input":"2021-11-24T18:10:05.820862Z","iopub.status.idle":"2021-11-24T18:10:06.548275Z","shell.execute_reply.started":"2021-11-24T18:10:05.820826Z","shell.execute_reply":"2021-11-24T18:10:06.547264Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Now let's use a homemade function found on the module `viz_utils.py` for looking at each dataset and bring some detailed parameters about the data content. With this function we can get the following information for each dataset column:\n\n    - Column name;\n    - Null amount;\n    - Null percentage among the respective dataset;\n    - Data type;\n    - total categorical entries;","metadata":{}},{"cell_type":"code","source":"df_overview = pd.DataFrame({})\nname_col = []\nfor name, df in zip(names, datasets):\n    name_col += [name] * df.shape[1]\n    df_overview = df_overview.append(data_overview(df))\n    df_overview['dataset_name'] = name_col\n\ndf_overview = df_overview.loc[:, ['dataset_name', 'feature', 'qtd_null', 'percent_null', 'dtype', 'qtd_cat']]\ndf_overview","metadata":{"ExecuteTime":{"start_time":"2020-07-05T23:57:15.60843Z","end_time":"2020-07-05T23:57:19.330198Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:06.552191Z","iopub.execute_input":"2021-11-24T18:10:06.552509Z","iopub.status.idle":"2021-11-24T18:10:08.354519Z","shell.execute_reply.started":"2021-11-24T18:10:06.552455Z","shell.execute_reply":"2021-11-24T18:10:08.353642Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Well, we can now use the DataFrame above wherever we want to do dome data transformation or data analysis. It contains basically the main information about each column for each one of the datasets available. This is very useful!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>3. Exploratory Data Analysis</b></font>","metadata":{}},{"cell_type":"markdown","source":"So now we will go trough an exploratory data analysis to get insights from E-Commerce in Brazil. The aim here is to divide this session into topics so we can explore graphics for each subject (orders, customers, products, items, and others).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.1 Total Orders on E-Commerce</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"We know that e-commerce is really a growing trend in a global perspective. Let's dive into the orders dataset to see how this trend can be presented in Brazil, at least on the dataset range.\n\nLooking at the dataset columns, we can see orders with different `status` and with different timestamp columns like `purchase`, `approved`, `delivered` and `estimated delivery`. First, let's look at the status of the orders we have in this dataset.","metadata":{}},{"cell_type":"markdown","source":"___\n* _How many orders we have for each status?_\n___","metadata":{}},{"cell_type":"code","source":"df_orders = olist_orders.merge(olist_customer, how='left', on='customer_id')\nfig, ax = plt.subplots(figsize=(14, 6))\nsingle_countplot(df_orders, x='order_status', ax=ax)\nplt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-05T23:57:19.334794Z","end_time":"2020-07-05T23:57:19.908832Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:08.357365Z","iopub.execute_input":"2021-11-24T18:10:08.357657Z","iopub.status.idle":"2021-11-24T18:10:08.928238Z","shell.execute_reply.started":"2021-11-24T18:10:08.357601Z","shell.execute_reply":"2021-11-24T18:10:08.927112Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"By the time this dataset was created, the highest amount of orders went from delivered ones. Only 3% of all orders came from the other status.","metadata":{}},{"cell_type":"markdown","source":"___\n* _Is that a growing trend on e-commerce in Brazil? How can we describe a complete scenario?_\n___","metadata":{}},{"cell_type":"markdown","source":"For the next plots, let's dive into the real evolution of e-commerce in terms of purchase orders. For this, we have to extract some info on the `order_purchase_timestamp` following the topics:\n\n    1. Transform timestamp columns;\n    2. Extract time attributes from these datetime columns (year, month, day, day of week and hour);\n    3. Evaluate the e-commerce scenario using this attributes.","metadata":{}},{"cell_type":"code","source":"# Changing the data type for date columns\ntimestamp_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', \n                  'order_estimated_delivery_date']\nfor col in timestamp_cols:\n    df_orders[col] = pd.to_datetime(df_orders[col])\n    \n# Extracting attributes for purchase date - Year and Month\ndf_orders['order_purchase_year'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.year)\ndf_orders['order_purchase_month'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.month)\ndf_orders['order_purchase_month_name'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%b'))\ndf_orders['order_purchase_year_month'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%Y%m'))\ndf_orders['order_purchase_date'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%Y%m%d'))\n\n# Extracting attributes for purchase date - Day and Day of Week\ndf_orders['order_purchase_day'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.day)\ndf_orders['order_purchase_dayofweek'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.dayofweek)\ndf_orders['order_purchase_dayofweek_name'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.strftime('%a'))\n\n# Extracting attributes for purchase date - Hour and Time of the Day\ndf_orders['order_purchase_hour'] = df_orders['order_purchase_timestamp'].apply(lambda x: x.hour)\nhours_bins = [-0.1, 6, 12, 18, 23]\nhours_labels = ['Dawn', 'Morning', 'Afternoon', 'Night']\ndf_orders['order_purchase_time_day'] = pd.cut(df_orders['order_purchase_hour'], hours_bins, labels=hours_labels)\n\n# New DataFrame after transformations\ndf_orders.head()","metadata":{"ExecuteTime":{"start_time":"2020-07-05T23:57:19.913149Z","end_time":"2020-07-05T23:57:27.244669Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:08.930144Z","iopub.execute_input":"2021-11-24T18:10:08.930997Z","iopub.status.idle":"2021-11-24T18:10:14.344727Z","shell.execute_reply.started":"2021-11-24T18:10:08.930916Z","shell.execute_reply":"2021-11-24T18:10:14.343518Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"So now we can purpose a complete analysis on orders amount of brazilian e-commerce during the period of the dataset. For that let's plot three graphs using a `GridSpec` with the aim answear the following questions:\n\n    1. Is there any growing trend on brazilian e-commerce?\n    2. On what day of week brazilians customers tend to do online purchasing?\n    3. What time brazilians customers tend do buy (Dawn, Morning, Afternoon or Night)?","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(13, 10))\n\n# Axis definition\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, :])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1])\n\n# Lineplot - Evolution of e-commerce orders along time \nsns.lineplot(data=df_orders['order_purchase_year_month'].value_counts().sort_index(), ax=ax1, \n             color='darkslateblue', linewidth=2)\nax1.annotate(f'Highest orders \\nreceived', (13, 7500), xytext=(-75, -25), \n             textcoords='offset points', bbox=dict(boxstyle=\"round4\", fc=\"w\", pad=.8),\n             arrowprops=dict(arrowstyle='-|>', fc='w'), color='dimgrey', ha='center')\nax1.annotate(f'Noise on data \\n(huge decrease)', (23, 0), xytext=(48, 25), \n             textcoords='offset points', bbox=dict(boxstyle=\"round4\", fc=\"w\", pad=.5),\n             arrowprops=dict(arrowstyle='-|>', fc='w'), color='dimgrey', ha='center')\nformat_spines(ax1, right_border=False)  \nfor tick in ax1.get_xticklabels():\n    tick.set_rotation(45)\nax1.set_title('Evolution of Total Orders in Brazilian E-Commerce', size=14, color='dimgrey')\n\n# Barchart - Total of orders by day of week\nsingle_countplot(df_orders, x='order_purchase_dayofweek', ax=ax2, order=False, palette='YlGnBu')\nweekday_label = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\nax2.set_xticklabels(weekday_label)\nax2.set_title('Total Orders by Day of Week', size=14, color='dimgrey', pad=20)\n\n# Barchart - Total of orders by time of the day\nday_color_list = ['darkslateblue', 'deepskyblue', 'darkorange', 'purple']\nsingle_countplot(df_orders, x='order_purchase_time_day', ax=ax3, order=False, palette=day_color_list)\nax3.set_title('Total Orders by Time of the Day', size=14, color='dimgrey', pad=20)\n\nplt.tight_layout()\nplt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-06T01:25:35.926179Z","end_time":"2020-07-06T01:25:36.682096Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:14.346121Z","iopub.execute_input":"2021-11-24T18:10:14.346580Z","iopub.status.idle":"2021-11-24T18:10:15.639319Z","shell.execute_reply.started":"2021-11-24T18:10:14.346533Z","shell.execute_reply":"2021-11-24T18:10:15.636176Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"By the chart above we can conclude:\n\n* E-commerce on Brazil really has a growing trend along the time. We can see some seasonality with peaks at specific months, but in general we can see clear that customers are more prone to buy things online than before.\n* Monday are the prefered day for brazilian's customers and they tend to buy more at afternoons.\n\n_Obs: we have a sharp decrease between August 2018 and September 2018 and maybe the origin of that is related to noise on data. For further comparison between 2017 and 2018, let's just consider orders between January and August in both years_","metadata":{}},{"cell_type":"markdown","source":"___\n* _E-commerce: a comparison between 2017 and 2018_\n___","metadata":{}},{"cell_type":"code","source":"# Creating figure\nfig = plt.figure(constrained_layout=True, figsize=(13, 5))\n\n# Axis definition\ngs = GridSpec(1, 3, figure=fig)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1:])\n\n# Annotation - Grown on e-commerce orders between 2017 and 2018\ndf_orders_compare = df_orders.query('order_purchase_year in (2017, 2018) & order_purchase_month <= 8')\nyear_orders = df_orders_compare['order_purchase_year'].value_counts()\ngrowth = int(round(100 * (1 + year_orders[2017] / year_orders[2018]), 0))\nax1.text(0.00, 0.73, f'{year_orders[2017]}', fontsize=40, color='mediumseagreen', ha='center')\nax1.text(0.00, 0.64, 'orders registered in 2017\\nbetween January and August', fontsize=10, ha='center')\nax1.text(0.00, 0.40, f'{year_orders[2018]}', fontsize=60, color='darkslateblue', ha='center')\nax1.text(0.00, 0.31, 'orders registered in 2018\\nbetween January and August', fontsize=10, ha='center')\nsignal = '+' if growth > 0 else '-'\nax1.text(0.00, 0.20, f'{signal}{growth}%', fontsize=14, ha='center', color='white', style='italic', weight='bold',\n         bbox=dict(facecolor='navy', alpha=0.5, pad=10, boxstyle='round, pad=.7'))\nax1.axis('off')\n\n# Bar chart - Comparison between monthly sales between 2017 and 2018\nsingle_countplot(df_orders_compare, x='order_purchase_month', hue='order_purchase_year', ax=ax2, order=False,\n                 palette='YlGnBu')\nmonth_label = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug']\nax2.set_xticklabels(month_label)\nax2.set_title('Total Orders Comparison Between 2017 and 2018 (January to August)', size=12, color='dimgrey', pad=20)\nplt.legend(loc='lower right')\nplt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-06T01:24:52.051006Z","end_time":"2020-07-06T01:24:52.505588Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:15.640680Z","iopub.execute_input":"2021-11-24T18:10:15.641181Z","iopub.status.idle":"2021-11-24T18:10:16.661777Z","shell.execute_reply.started":"2021-11-24T18:10:15.641128Z","shell.execute_reply":"2021-11-24T18:10:16.660729Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.2 E-Commerce Around Brazil</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"For preparing the data to a workaround analysis on brazilian's states e-commerce, we will take the following steps:\n\n    1. Merge the orders data to order_items data;\n    2. Use an API (brazilian government) to return the region of each customer_state;\n    3. Purpose useful charts to answear business questions.","metadata":{}},{"cell_type":"markdown","source":"* Brazilian APIs ans links for geolocation info:\n\n        https://servicodados.ibge.gov.br/api/docs/localidades?versao=1","metadata":{}},{"cell_type":"markdown","source":"Thanks to Andre Sionek that threated the geolocation lat and long on his kernel!","metadata":{}},{"cell_type":"code","source":"# Merging orders and order_items\ndf_orders_items = df_orders.merge(olist_order_items, how='left', on='order_id')\n\n# Using the API to bring the region to the data\nr = requests.get('https://servicodados.ibge.gov.br/api/v1/localidades/mesorregioes')\ncontent = [c['UF'] for c in json.loads(r.text)]\nbr_info = pd.DataFrame(content)\nbr_info['nome_regiao'] = br_info['regiao'].apply(lambda x: x['nome'])\nbr_info.drop('regiao', axis=1, inplace=True)\nbr_info.drop_duplicates(inplace=True)\n\n# Threting geolocations outside brazilian map\n\n#Brazils most Northern spot is at 5 deg 16′ 27.8″ N latitude.;\ngeo_prep = olist_geolocation[olist_geolocation.geolocation_lat <= 5.27438888]\n#it’s most Western spot is at 73 deg, 58′ 58.19″W Long.\ngeo_prep = geo_prep[geo_prep.geolocation_lng >= -73.98283055]\n#It’s most southern spot is at 33 deg, 45′ 04.21″ S Latitude.\ngeo_prep = geo_prep[geo_prep.geolocation_lat >= -33.75116944]\n#It’s most Eastern spot is 34 deg, 47′ 35.33″ W Long.\ngeo_prep = geo_prep[geo_prep.geolocation_lng <=  -34.79314722]\ngeo_group = geo_prep.groupby(by='geolocation_zip_code_prefix', as_index=False).min()\n\n# Merging all the informations\ndf_orders_items = df_orders_items.merge(br_info, how='left', left_on='customer_state', right_on='sigla')\ndf_orders_items = df_orders_items.merge(geo_group, how='left', left_on='customer_zip_code_prefix', \n                                        right_on='geolocation_zip_code_prefix')\ndf_orders_items.head()","metadata":{"ExecuteTime":{"end_time":"2020-07-11T16:31:26.638178Z","start_time":"2020-07-11T16:31:14.696897Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:16.663218Z","iopub.execute_input":"2021-11-24T18:10:16.663522Z","iopub.status.idle":"2021-11-24T18:10:29.404445Z","shell.execute_reply.started":"2021-11-24T18:10:16.663475Z","shell.execute_reply":"2021-11-24T18:10:29.403607Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"___\n* _An overview of customer's order by region, state and city_\n___","metadata":{}},{"cell_type":"code","source":"# Filtering data between 201701 and 201808\ndf_orders_filt = df_orders_items[(df_orders_items['order_purchase_year_month'].astype(int) >= 201701)]\ndf_orders_filt = df_orders_filt[(df_orders_filt['order_purchase_year_month'].astype(int) <= 201808)]\n\n# Grouping data by region\ndf_regions_group = df_orders_filt.groupby(by=['order_purchase_year_month', 'nome_regiao'], as_index=False)\ndf_regions_group = df_regions_group.agg({'customer_id': 'count', 'price': 'sum'}).sort_values(by='order_purchase_year_month')\ndf_regions_group.columns = ['month', 'region', 'order_count', 'order_amount']\ndf_regions_group.reset_index(drop=True, inplace=True)\n\n# Grouping data by city (top 10)\ndf_cities_group = df_orders_filt.groupby(by='geolocation_city', \n                                       as_index=False).count().loc[:, ['geolocation_city', 'order_id']]\ndf_cities_group = df_cities_group.sort_values(by='order_id', ascending=False).reset_index(drop=True)\ndf_cities_group = df_cities_group.iloc[:10, :]","metadata":{"ExecuteTime":{"end_time":"2020-07-11T16:33:24.676254Z","start_time":"2020-07-11T16:33:24.278294Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:29.405494Z","iopub.execute_input":"2021-11-24T18:10:29.405855Z","iopub.status.idle":"2021-11-24T18:10:29.810983Z","shell.execute_reply.started":"2021-11-24T18:10:29.405816Z","shell.execute_reply":"2021-11-24T18:10:29.810106Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Creating and preparing figure and axis\nfig = plt.figure(constrained_layout=True, figsize=(15, 10))\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[:, 1])\n\n# Count of orders by region\nsns.lineplot(x='month', y='order_count', ax=ax1, data=df_regions_group, hue='region', \n             size='region', style='region', palette='magma', markers=['o'] * 5)\nformat_spines(ax1, right_border=False)\nax1.set_title('Evolution of E-Commerce Orders on Brazilian Regions', size=12, color='dimgrey')\nax1.set_ylabel('')\nfor tick in ax1.get_xticklabels():\n    tick.set_rotation(45)\n\n# Top cities with more customers orders in Brazil\nsns.barplot(y='geolocation_city', x='order_id', data=df_cities_group, ax=ax2, palette='magma')\nAnnotateBars(n_dec=0, font_size=10, color='black').horizontal(ax2)\nformat_spines(ax2, right_border=False)\nax2.set_title('Top 10 Brazilian Cities with More Orders', size=12, color='dimgrey')\nax2.set_ylabel('')\n\n# Total orders by state\nsingle_countplot(y='customer_state', ax=ax3, df=df_orders_filt, palette='viridis')\nax3.set_title('Total of Customers Orders by State', size=12, color='dimgrey')\nax3.set_ylabel('')\n\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2020-07-11T16:33:38.535124Z","start_time":"2020-07-11T16:33:36.18223Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:29.812568Z","iopub.execute_input":"2021-11-24T18:10:29.813036Z","iopub.status.idle":"2021-11-24T18:10:32.962248Z","shell.execute_reply.started":"2021-11-24T18:10:29.812911Z","shell.execute_reply":"2021-11-24T18:10:32.961272Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"___\n* _How customers are distributed in Brazil? (a 30k orders sample from 2018 in a map)_\n___","metadata":{}},{"cell_type":"code","source":"# Zipping locations\nlats = list(df_orders_items.query('order_purchase_year == 2018')['geolocation_lat'].dropna().values)[:30000]\nlongs = list(df_orders_items.query('order_purchase_year == 2018')['geolocation_lng'].dropna().values)[:30000]\nlocations = list(zip(lats, longs))\n\n# Creating a mapa using folium\nmap1 = folium.Map(location=[-15, -50], zoom_start=4.0)\n\n# Plugin: FastMarkerCluster\nFastMarkerCluster(data=locations).add_to(map1)\n\nmap1","metadata":{"ExecuteTime":{"end_time":"2020-07-11T16:33:43.457128Z","start_time":"2020-07-11T16:33:42.554465Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:32.963742Z","iopub.execute_input":"2021-11-24T18:10:32.964007Z","iopub.status.idle":"2021-11-24T18:10:33.476004Z","shell.execute_reply.started":"2021-11-24T18:10:32.963960Z","shell.execute_reply":"2021-11-24T18:10:33.474734Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"___\n* _**HeatMaps:** a good view to see where are the core of brazilian e-commerce customers_\n___","metadata":{}},{"cell_type":"markdown","source":"By the map we showed above, we have already the insight that the southeast of Brazil has the highest number of orders given through e-commerce. So, let's see it in a HeatMap!","metadata":{}},{"cell_type":"code","source":"# Grouping geolocation data for plotting a heatmap\nheat_data = df_orders_filt.groupby(by=['geolocation_lat', 'geolocation_lng'], as_index=False).count().iloc[:, :3]\n\n# Creating a mapa using folium\nmap1 = folium.Map(\n    location=[-15, -50], \n    zoom_start=4.0, \n    tiles='cartodbdark_matter'\n)\n\n# Plugin: HeatMap\nHeatMap(\n    name='Mapa de Calor',\n    data=heat_data,\n    radius=10,\n    max_zoom=13\n).add_to(map1)\n\nmap1","metadata":{"ExecuteTime":{"start_time":"2020-07-12T21:32:15.767687Z","end_time":"2020-07-12T21:32:16.382557Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:33.477294Z","iopub.execute_input":"2021-11-24T18:10:33.477534Z","iopub.status.idle":"2021-11-24T18:10:33.941512Z","shell.execute_reply.started":"2021-11-24T18:10:33.477490Z","shell.execute_reply":"2021-11-24T18:10:33.940544Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**Nice!** Another good view is to use the folium plugin _[HeatMapWithTime](https://nbviewer.jupyter.org/github/python-visualization/folium/blob/master/examples/HeatMapWithTime.ipynb)_ to see the evolution of e-commerce orders among time.\n\nFor [limitations](https://github.com/python-visualization/folium/issues/859) purpose (i.e. jupyter and Chrome limitations for total number of points shown at HeatMapWithTime, we will show the evolution of orders from January 2018 to July 2018)\n\nAlso, it's possible that the plugin HeatMapWithTime doesn't work properly from a [issue](https://github.com/python-visualization/folium/issues/1221) fixed on version 0.11 (it's seems that the version of the kernel is 0.10). It it is the case for you, just updating the version of folium library would fix it.","metadata":{}},{"cell_type":"code","source":"epoch_list = []\nheatmap_evl_data = df_orders_items[(df_orders_items['order_purchase_year_month'].astype(int) >= 201801)]\nheatmap_evl_data = heatmap_evl_data[(heatmap_evl_data['order_purchase_year_month'].astype(int) <= 201807)]\ntime_index = heatmap_evl_data['order_purchase_year_month'].sort_values().unique()\nfor epoch in time_index:\n    data_temp = heatmap_evl_data.query('order_purchase_year_month == @epoch')\n    data_temp = data_temp.groupby(by=['geolocation_lat', 'geolocation_lng'], as_index=False).count()\n    data_temp = data_temp.sort_values(by='order_id', ascending=False).iloc[:, :3]\n    epoch_list.append(data_temp.values.tolist())\n    \n# Creating a mapa using folium\nmap2 = folium.Map(\n    location=[-15, -50], \n    zoom_start=4.0, \n    tiles='cartodbdark_matter'\n)\n\n# Plugin: HeatMapWithTime\nHeatMapWithTime(\n    name='Evolution of Orders in a Geolocation Perspective',\n    data=epoch_list,\n    radius=10,\n    index=list(time_index)\n).add_to(map2)\n\nmap2","metadata":{"ExecuteTime":{"start_time":"2020-07-12T21:49:05.114419Z","end_time":"2020-07-12T21:49:06.352749Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:33.943192Z","iopub.execute_input":"2021-11-24T18:10:33.943500Z","iopub.status.idle":"2021-11-24T18:10:34.881194Z","shell.execute_reply.started":"2021-11-24T18:10:33.943436Z","shell.execute_reply":"2021-11-24T18:10:34.880401Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.3 E-Commerce Impact on Economy</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Until now, we just answered questions on E-commerce scenario considering the number of orders received. We could see the volumetry amonth months, day of week, time of the day and even the geolocation states.\n\nNow, we will analyze the money movemented by e-commerce by looking at order prices, freights and others.","metadata":{}},{"cell_type":"markdown","source":"___\n* _How about the e-commerce sales? Did they grow up over time?_\n___","metadata":{}},{"cell_type":"markdown","source":"For answear this question, let's first group our data in a way to look at the evolution overall.","metadata":{}},{"cell_type":"code","source":"# Grouping data\ndf_month_aggreg = df_orders_filt.groupby(by=['order_purchase_year', 'order_purchase_year_month'], as_index=False)\ndf_month_aggreg = df_month_aggreg.agg({\n    'order_id': 'count',\n    'price': 'sum',\n    'freight_value': 'sum'\n})\n\n# Adding new columns for analysis\ndf_month_aggreg['price_per_order'] = df_month_aggreg['price'] / df_month_aggreg['order_id']\ndf_month_aggreg['freight_per_order'] = df_month_aggreg['freight_value'] / df_month_aggreg['order_id']\ndf_month_aggreg.head()","metadata":{"ExecuteTime":{"start_time":"2020-07-16T02:13:54.002213Z","end_time":"2020-07-16T02:13:54.055071Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:34.882616Z","iopub.execute_input":"2021-11-24T18:10:34.882872Z","iopub.status.idle":"2021-11-24T18:10:34.923209Z","shell.execute_reply.started":"2021-11-24T18:10:34.882829Z","shell.execute_reply":"2021-11-24T18:10:34.922167Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Creating figure\nfig = plt.figure(constrained_layout=True, figsize=(15, 12))\n\n# Axis definition\ngs = GridSpec(2, 3, figure=fig)\nax1 = fig.add_subplot(gs[0, :])\nax2 = fig.add_subplot(gs[1, 0])\nax3 = fig.add_subplot(gs[1, 1:])\n\n# Plot 1 - Evolution of total orders and total sales on e-commerce\nsns.lineplot(x='order_purchase_year_month', y='price', ax=ax1, data=df_month_aggreg, linewidth=2, \n             color='darkslateblue', marker='o', label='Total Amount')\nax1_twx = ax1.twinx()\nsingle_countplot(df_orders_filt, x='order_purchase_year_month', ax=ax1_twx, order=False, palette='YlGnBu_r')\nax1_twx.set_yticks(np.arange(0, 20000, 2500))\n\n# Customizing the first plot\nformat_spines(ax1)\nfor tick in ax1.get_xticklabels():\n    tick.set_rotation(45)\nfor x, y in df_month_aggreg.price.items():\n    ax1.annotate(str(round(y/1000, 1))+'K', xy=(x, y), textcoords='offset points', xytext=(0, 10),\n                ha='center', color='dimgrey')\nax1.annotate(f'Highest Value Sold on History\\n(Black Friday?)', (10, 1000000), xytext=(-120, -20), \n             textcoords='offset points', bbox=dict(boxstyle=\"round4\", fc=\"w\", pad=.8),\n             arrowprops=dict(arrowstyle='-|>', fc='w'), color='dimgrey', ha='center')\nax1.set_title('Evolution of E-commerce: Total Orders and Total Amount Sold (R$)', size=14, color='dimgrey', pad=20)\n\n# Plot 2 - Big Numbers of Sales Evolution\nmonth_comparison = ['201701', '201702', '201703', '201704', '201705', '201706', '201707', '201708',\n                    '201801', '201802', '201803', '201804', '201805', '201806', '201807', '201808']\ndf_sales_compare = df_month_aggreg.query('order_purchase_year_month in (@month_comparison)')\nsold_2017 = df_sales_compare.query('order_purchase_year == 2017')['price'].sum()\nsold_2018 = df_sales_compare.query('order_purchase_year == 2018')['price'].sum()\ngrowth = 1 + (sold_2017 / sold_2018)\nax2.text(0.50, 0.73, 'R$' + str(round(sold_2017/1000000, 2)) + 'M', fontsize=40, color='mediumseagreen', ha='center')\nax2.text(0.50, 0.60, 'total amount sold in 2017\\nbetween January and August', fontsize=10, ha='center')\nax2.text(0.50, 0.36, 'R$' + str(round(sold_2018/1000000, 2)) + 'M', fontsize=60, color='darkslateblue', ha='center')\nax2.text(0.50, 0.25, 'total amount sold in 2018\\nbetween January and August', fontsize=10, ha='center')\nsignal = '+' if growth > 0 else '-'\nax2.text(0.50, 0.13, f'{signal}{str(round(100 * growth, 2))}%', fontsize=14, ha='center', color='white', style='italic', weight='bold',\n         bbox=dict(facecolor='navy', alpha=0.5, pad=10, boxstyle='round, pad=.7'))\nax2.axis('off')\n\n# Plot 3 - Evolution of mean freight value paid by the customers\nsns.lineplot(x='order_purchase_year_month', y='freight_per_order', data=df_month_aggreg, linewidth=2, \n             color='silver', marker='o', ax=ax3)\nformat_spines(ax3, right_border=False)\nfor tick in ax3.get_xticklabels():\n    tick.set_rotation(45)\nfor x, y in df_month_aggreg.freight_per_order.items():\n    ax3.annotate(round(y, 2), xy=(x, y), textcoords='offset points', xytext=(0, 10),\n                ha='center', color='dimgrey')\nax3.set_title('Evolution of Average Freight Value (RS) Paid by Customers', size=14, color='dimgrey', pad=20)\n\nplt.tight_layout()\nplt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-16T02:13:54.058063Z","end_time":"2020-07-16T02:13:57.671356Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:34.924580Z","iopub.execute_input":"2021-11-24T18:10:34.924849Z","iopub.status.idle":"2021-11-24T18:10:36.741054Z","shell.execute_reply.started":"2021-11-24T18:10:34.924800Z","shell.execute_reply":"2021-11-24T18:10:36.740145Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"___\n* _How the total sales (sum of price) are concentraded in brazilian states?_\n___","metadata":{}},{"cell_type":"code","source":"mean_sum_analysis(df_orders_filt, 'customer_state', 'price', palette='viridis', figsize=(15, 10))","metadata":{"ExecuteTime":{"start_time":"2020-07-18T02:28:42.891782Z","end_time":"2020-07-18T02:28:44.183881Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:36.742575Z","iopub.execute_input":"2021-11-24T18:10:36.742849Z","iopub.status.idle":"2021-11-24T18:10:38.283870Z","shell.execute_reply.started":"2021-11-24T18:10:36.742797Z","shell.execute_reply":"2021-11-24T18:10:38.283199Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"It's very interesting to see how some states have a high total amount sold and a low price per order. If we look at SP (São Paulo) for example, it's possible to see that it is the state with most valuable state for e-commerce (5,188,099 sold) but it is also where customers pay less per order (110.00 per order).","metadata":{}},{"cell_type":"markdown","source":"How about the freight?","metadata":{}},{"cell_type":"code","source":"mean_sum_analysis(df_orders_filt, 'customer_state', 'freight_value', palette='viridis', figsize=(15, 10))","metadata":{"ExecuteTime":{"start_time":"2020-07-18T02:28:44.185837Z","end_time":"2020-07-18T02:28:45.290852Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:38.284928Z","iopub.execute_input":"2021-11-24T18:10:38.285368Z","iopub.status.idle":"2021-11-24T18:10:39.776720Z","shell.execute_reply.started":"2021-11-24T18:10:38.285303Z","shell.execute_reply":"2021-11-24T18:10:39.775715Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Here we can get insights about the customers states with highest mean freight value. For example, customers in Roraima (RR), Paraíba (PB), Rondônia (RO) and Acre (AC) normaly pays more than anyone on freights.","metadata":{"ExecuteTime":{"end_time":"2020-07-16T16:41:09.180095Z","start_time":"2020-07-16T16:41:09.172022Z"}}},{"cell_type":"markdown","source":"___\n* _What are the best states to buy in Brazil? An analysis on sales, freight and delivery time_\n___","metadata":{}},{"cell_type":"code","source":"# Calculating working days between purchasing, delivering and estimated delivery\npurchasing = df_orders_filt['order_purchase_timestamp']\ndelivered = df_orders_filt['order_delivered_customer_date']\nestimated = df_orders_filt['order_estimated_delivery_date']\ndf_orders_filt['time_to_delivery'] = calc_working_days(purchasing, delivered, convert=True)\ndf_orders_filt['diff_estimated_delivery'] = calc_working_days(estimated, delivered, convert=True)\n\n# Grouping data by state\nstates_avg_grouped = df_orders_filt.groupby(by='customer_state', as_index=False).mean()\nstates_freight_paid = states_avg_grouped.loc[:, ['customer_state', 'freight_value']]\nstates_time_to_delivery = states_avg_grouped.loc[:, ['customer_state', 'time_to_delivery']]\nstates_estimated_delivery = states_avg_grouped.loc[:, ['customer_state', 'diff_estimated_delivery']]\n\n# Sorting data\nstates_freight_paid = states_freight_paid.sort_values(by='freight_value', ascending=False)\nstates_time_to_delivery = states_time_to_delivery.sort_values(by='time_to_delivery', ascending=False)\nstates_estimated_delivery = states_estimated_delivery.sort_values(by='diff_estimated_delivery')","metadata":{"ExecuteTime":{"start_time":"2020-07-22T16:09:43.874021Z","end_time":"2020-07-22T16:09:46.054811Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:39.778188Z","iopub.execute_input":"2021-11-24T18:10:39.778456Z","iopub.status.idle":"2021-11-24T18:10:40.910284Z","shell.execute_reply.started":"2021-11-24T18:10:39.778405Z","shell.execute_reply":"2021-11-24T18:10:40.909184Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n\n# Plot Pack 01 - Freight value paid on states\nsns.barplot(x='freight_value', y='customer_state', data=states_freight_paid.head(), ax=axs[1, 0], palette='viridis')\naxs[1, 0].set_title('Top 5 States with Highest \\nAverage Freight Value', size=12, color='black')\nsns.barplot(x='freight_value', y='customer_state', data=states_freight_paid.tail(), ax=axs[2, 0], palette='viridis_r')\naxs[2, 0].set_title('Top 5 States with Lowest \\nAverage Freight Value', size=12, color='black')\nfor ax in axs[1, 0], axs[2, 0]:\n    ax.set_xlabel('Mean Freight Value')\n    ax.set_xlim(0, states_freight_paid['freight_value'].max())\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n\n# Annotations\naxs[0, 0].text(0.50, 0.30, f'R${round(df_orders_filt.freight_value.mean(), 2)}', fontsize=45, ha='center')\naxs[0, 0].text(0.50, 0.12, 'is the mean value of freight paid', fontsize=12, ha='center')\naxs[0, 0].text(0.50, 0.00, 'for online shopping', fontsize=12, ha='center')\naxs[0, 0].axis('off')\n\n# Plot Pack 02 - Time to delivery on states\nsns.barplot(x='time_to_delivery', y='customer_state', data=states_time_to_delivery.head(), ax=axs[1, 1], palette='viridis')\naxs[1, 1].set_title('Top 5 States with Highest \\nAverage Time to Delivery', size=12, color='black')\nsns.barplot(x='time_to_delivery', y='customer_state', data=states_time_to_delivery.tail(), ax=axs[2, 1], palette='viridis_r')\naxs[2, 1].set_title('Top 5 States with Lowest \\nAverage Time do Delivery', size=12, color='black')\nfor ax in axs[1, 1], axs[2, 1]:\n    ax.set_xlabel('Time to Delivery')\n    ax.set_xlim(0, states_time_to_delivery['time_to_delivery'].max())\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n    \n# Annotations\naxs[0, 1].text(0.40, 0.30, f'{int(df_orders_filt.time_to_delivery.mean())}', fontsize=45, ha='center')\naxs[0, 1].text(0.60, 0.30, 'working days', fontsize=12, ha='center')\naxs[0, 1].text(0.50, 0.12, 'is the average delay for delivery', fontsize=12, ha='center')\naxs[0, 1].text(0.50, 0.00, 'for online shopping', fontsize=12, ha='center')\naxs[0, 1].axis('off')\n    \n# Plot Pack 03 - Differnece between delivered and estimated on states\nsns.barplot(x='diff_estimated_delivery', y='customer_state', data=states_estimated_delivery.head(), ax=axs[1, 2], palette='viridis')\naxs[1, 2].set_title('Top 5 States where Delivery is \\nReally Fast Comparing to Estimated', size=12, color='black')\nsns.barplot(x='diff_estimated_delivery', y='customer_state', data=states_estimated_delivery.tail(), ax=axs[2, 2], palette='viridis_r')\naxs[2, 2].set_title('Top 5 States where Delivery is \\nNot So Fast Comparing to Estimated', size=12, color='black')\nfor ax in axs[1, 2], axs[2, 2]:\n    ax.set_xlabel('Days Between Delivery and Estimated')\n    ax.set_xlim(states_estimated_delivery['diff_estimated_delivery'].min()-1, \n                states_estimated_delivery['diff_estimated_delivery'].max()+1)\n    format_spines(ax, right_border=False)\n    ax.set_ylabel('')\n\n# Annotations\naxs[0, 2].text(0.40, 0.30, f'{int(df_orders_filt.diff_estimated_delivery.mean())}', fontsize=45, ha='center')\naxs[0, 2].text(0.60, 0.30, 'working days', fontsize=12, ha='center')\naxs[0, 2].text(0.50, 0.12, 'is the average difference between', fontsize=12, ha='center')\naxs[0, 2].text(0.50, 0.00, 'delivery and estimated date', fontsize=12, ha='center')\naxs[0, 2].axis('off') \n    \nplt.suptitle('Comparative Study: E-Commerce on Brazilian States', size=16)\nplt.tight_layout()\nplt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-22T16:09:46.059348Z","end_time":"2020-07-22T16:09:48.093942Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:40.911674Z","iopub.execute_input":"2021-11-24T18:10:40.911926Z","iopub.status.idle":"2021-11-24T18:10:42.838371Z","shell.execute_reply.started":"2021-11-24T18:10:40.911882Z","shell.execute_reply":"2021-11-24T18:10:42.837478Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.4 Payment Type Analysis</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"One of the datasets provided have informations about order's payment. To see how payments can take influence on e-commerce, we can build a mini-dashboard with main concepts: `payments type` and `payments installments`. The idea is to present enough information to clarify how e-commerce buyers usually prefer to pay orders.","metadata":{}},{"cell_type":"code","source":"# Grouping data\ndf_orders_pay = df_orders_filt.merge(olist_order_payments, how='left', on='order_id')\n\n# Creating figure\nfig = plt.figure(constrained_layout=True, figsize=(15, 12))\n\n# Axis definition\ngs = GridSpec(2, 2, figure=fig)\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\nax3 = fig.add_subplot(gs[1, :])\n\n# Plot 1 - Payment types in a donut chart\ncolors = ['darkslateblue', 'cornflowerblue', 'silver', 'darkviolet', 'crimson']\nlabel_names = df_orders_pay['payment_type'].value_counts().index\ndonut_plot(df_orders_pay, col='payment_type', ax=ax1, label_names=label_names, colors=colors,\n           title='Count of Transactions by Payment Type', text=f'{len(df_orders_pay)}\\npayments \\nregistered')\n\n# Plot 2 - Payment installments\nsingle_countplot(df_orders_pay, ax=ax2, y='payment_installments')\nax2.set_title('A Distribution of Payment Installments', color='dimgrey', size=12)\n\n# Plot 3 - Evolution of payment types\npayment_evl = df_orders_pay.groupby(by=['order_purchase_year_month', 'payment_type'], as_index=False).count()\npayment_evl = payment_evl.loc[:, ['order_purchase_year_month', 'payment_type', 'order_id']]\npayment_evl = payment_evl.sort_values(by=['order_purchase_year_month', 'order_id'], ascending=[True, False])\nsns.lineplot(x='order_purchase_year_month', y='order_id', data=payment_evl, ax=ax3, hue='payment_type',\n             style='payment_type', size='payment_type', palette=colors, marker='o')\nformat_spines(ax3, right_border=False)\nax3.set_title('Evolution of Payment Types in Brazilian E-Commerce', size=12, color='dimgrey')\nplt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-23T00:35:16.777842Z","end_time":"2020-07-23T00:35:19.827457Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:42.839593Z","iopub.execute_input":"2021-11-24T18:10:42.839845Z","iopub.status.idle":"2021-11-24T18:10:46.459404Z","shell.execute_reply.started":"2021-11-24T18:10:42.839802Z","shell.execute_reply":"2021-11-24T18:10:46.458212Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"In fact, we can see by the line chart that payments made by credit card really took marjority place on brazilian e-commerce. Besides that, since 201803 it's possible to see a little decrease on this type of payment. By the other side, payments made by debit card is showing a growing trend since 201805, wich is a good opportunity for investor to improve services for payments like this.\n\nOn the bar chart above, we can see how brazilian customers prefer to pay the orders: mostly of them pay once into 1 installment and it's worth to point out the quantity of payments done by 10 installments.","metadata":{}},{"cell_type":"markdown","source":"**Note: With this, we close our session for EDA and now we can start the Natural Language Process step on reviews! Keep in touch for more!**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>4. Natural Language Processing</b></font>","metadata":{}},{"cell_type":"markdown","source":"As long as we could improve our relationship with the data, the path is open to start the Natural Language Processing step to analyze the comments left on e-commerce orders. The goal is to use this as input to a `sentimental analysis` model for understanding the customer's sentiment on purchasing things online. Let's take a look on the reviews data.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.1 Data Understanding</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"df_comments = olist_order_reviews.loc[:, ['review_score', 'review_comment_message']]\ndf_comments = df_comments.dropna(subset=['review_comment_message'])\ndf_comments = df_comments.reset_index(drop=True)\nprint(f'Dataset shape: {df_comments.shape}')\ndf_comments.columns = ['score', 'comment']\ndf_comments.head()","metadata":{"ExecuteTime":{"start_time":"2020-07-24T02:36:21.64127Z","end_time":"2020-07-24T02:36:21.682161Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:46.460627Z","iopub.execute_input":"2021-11-24T18:10:46.460872Z","iopub.status.idle":"2021-11-24T18:10:46.493653Z","shell.execute_reply.started":"2021-11-24T18:10:46.460830Z","shell.execute_reply":"2021-11-24T18:10:46.492687Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"So, we have in hands approximately 41k comments that could be used for training a sentimental analysis model. But, for this to becoming true, we have to go trough a long way of text preparation to transform the comment input into a vector that can be interpreted for a Machine Learning model. **Let's go ahead**","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.2 Regular Expressions</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"As long as we consider the global internet as the source of our comments, probably we have to deal with some HTML tags, break lines, special characteres and other content that could be part of the dataset. Let's dig a little bit more on `Regular Expressions` to search for those patterns.\n\nFirst of all, let's define a function that will be used for analysing the results of an applied regular expression. With whis we can validate our text pre processing in an easier way.","metadata":{}},{"cell_type":"code","source":"def find_patterns(re_pattern, text_list):\n    \"\"\"\n    Args:\n    ---------\n    re_pattern: regular expression pattern to be used on search [type: string]\n    text_list: list with text strings [type: list]\n    \n    Returns:\n    positions_dict: python dictionary with key-value pars as below:\n        text_idx: [(start_pattern1, end_pattern1), (start_pattern1, end_pattern2), ... (start_n, end_n)]\n    \"\"\"\n    \n    # Compiling the Regular Expression passed as a arg\n    p = re.compile(re_pattern)\n    positions_dict = {}\n    i = 0\n    for c in text_list:\n        match_list = []\n        iterator = p.finditer(c)\n        for match in iterator:\n            match_list.append(match.span())\n        control_key = f'Text idx {i}'\n        if len(match_list) == 0:\n            pass\n        else:\n            positions_dict[control_key] = match_list\n        i += 1\n        \n    \"\"\"p = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n    pattern_dict = find_patterns(p, reviews_breakline)\n    print(len(pattern_dict))\n    pattern_dict\n    for idx in [int(c.split(' ')[-1]) for c in list(pattern_dict.keys())]:\n        print(f'{reviews_breakline[idx]}\\n')\"\"\"\n\n    return positions_dict\n\ndef print_step_result(text_list_before, text_list_after, idx_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list_before: list object with text content before transformation [type: list]\n    text_list_after: list object with text content after transformation [type: list]\n    idx_list: list object with indexes to be printed [type: list]\n    \"\"\"\n    \n    # Iterating over string examples\n    i = 1\n    for idx in idx_list:\n        print(f'--- Text {i} ---\\n')\n        print(f'Before: \\n{text_list_before[idx]}\\n')\n        print(f'After: \\n{text_list_after[idx]}\\n')\n        i += 1","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:11.833724Z","end_time":"2020-07-25T16:22:11.849692Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:46.495693Z","iopub.execute_input":"2021-11-24T18:10:46.496206Z","iopub.status.idle":"2021-11-24T18:10:46.505413Z","shell.execute_reply.started":"2021-11-24T18:10:46.496019Z","shell.execute_reply":"2021-11-24T18:10:46.504533Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.1\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.1 Breakline and Carriage Return</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"def re_breakline(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('[\\n\\r]', ' ', r) for r in text_list]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:11.855819Z","end_time":"2020-07-25T16:22:11.864895Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:46.507036Z","iopub.execute_input":"2021-11-24T18:10:46.507360Z","iopub.status.idle":"2021-11-24T18:10:46.522993Z","shell.execute_reply.started":"2021-11-24T18:10:46.507307Z","shell.execute_reply":"2021-11-24T18:10:46.521967Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Creating a list of comment reviews\nreviews = list(df_comments['comment'].values)\n\n# Applying RegEx\nreviews_breakline = re_breakline(reviews)\ndf_comments['re_breakline'] = reviews_breakline\n\n# Verifying results\nprint_step_result(reviews, reviews_breakline, idx_list=[48])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:11.868002Z","end_time":"2020-07-25T16:22:12.048986Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:46.524448Z","iopub.execute_input":"2021-11-24T18:10:46.524696Z","iopub.status.idle":"2021-11-24T18:10:46.610360Z","shell.execute_reply.started":"2021-11-24T18:10:46.524660Z","shell.execute_reply":"2021-11-24T18:10:46.609640Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Here it's possible to see the tags \\r (_carriage return_ code ASCII 10) and \\n (_new line_ code ASCII 13). With RegEx, we could get rid of those patterns.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.2.2\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.2 Sites and Hiperlinks</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Another pattern that must be threated is sites and hiperlinks. Let's define another function to apply RegEx on this.","metadata":{}},{"cell_type":"code","source":"def re_hiperlinks(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    return [re.sub(pattern, ' link ', r) for r in text_list]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.05092Z","end_time":"2020-07-25T16:22:12.0645Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:46.611482Z","iopub.execute_input":"2021-11-24T18:10:46.611909Z","iopub.status.idle":"2021-11-24T18:10:46.616829Z","shell.execute_reply.started":"2021-11-24T18:10:46.611844Z","shell.execute_reply":"2021-11-24T18:10:46.615875Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Applying RegEx\nreviews_hiperlinks = re_hiperlinks(reviews_breakline)\ndf_comments['re_hiperlinks'] = reviews_hiperlinks\n\n# Verifying results\nprint_step_result(reviews_breakline, reviews_hiperlinks, idx_list=[10796, 12782])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.067685Z","end_time":"2020-07-25T16:22:12.188978Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:46.618280Z","iopub.execute_input":"2021-11-24T18:10:46.618711Z","iopub.status.idle":"2021-11-24T18:10:46.679182Z","shell.execute_reply.started":"2021-11-24T18:10:46.618665Z","shell.execute_reply":"2021-11-24T18:10:46.678293Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.3\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.3 Dates</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, as long as we are dealing with customers reviews on items bought online, probably date mentions are very common. Let's see some examples and apply a RegEx to change this to `data` (means `date` in english).","metadata":{}},{"cell_type":"code","source":"def re_dates(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    pattern = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}'\n    return [re.sub(pattern, ' data ', r) for r in text_list]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.197004Z","end_time":"2020-07-25T16:22:12.209258Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:46.680877Z","iopub.execute_input":"2021-11-24T18:10:46.681475Z","iopub.status.idle":"2021-11-24T18:10:46.687216Z","shell.execute_reply.started":"2021-11-24T18:10:46.681421Z","shell.execute_reply":"2021-11-24T18:10:46.686258Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Applying RegEx\nreviews_dates = re_dates(reviews_hiperlinks)\ndf_comments['re_dates'] = reviews_dates\n\n# Verifying results\nprint_step_result(reviews_hiperlinks, reviews_dates, idx_list=[26665, 41497, 41674])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.213175Z","end_time":"2020-07-25T16:22:12.570251Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:46.690129Z","iopub.execute_input":"2021-11-24T18:10:46.690697Z","iopub.status.idle":"2021-11-24T18:10:47.065455Z","shell.execute_reply.started":"2021-11-24T18:10:46.690472Z","shell.execute_reply":"2021-11-24T18:10:47.061616Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.4\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.4 Money</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Another pattern that probably is very common on this kind of source is representations of money (R$ \\__,\\__). To improve our model, maybe it's a good idea to transform this pattern into a key word like `valor` (means `money` or `amount` in english).","metadata":{}},{"cell_type":"code","source":"def re_money(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_list: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    pattern = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+'\n    return [re.sub(pattern, ' dinheiro ', r) for r in text_list]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.578848Z","end_time":"2020-07-25T16:22:12.604886Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.066581Z","iopub.status.idle":"2021-11-24T18:10:47.067245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying RegEx\nreviews_money = re_money(reviews_dates)\ndf_comments['re_money'] = reviews_money\n\n# Verifying results\nprint_step_result(reviews_dates, reviews_money, idx_list=[26020, 33297, 32998])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.608874Z","end_time":"2020-07-25T16:22:12.829989Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.068998Z","iopub.status.idle":"2021-11-24T18:10:47.069761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.5\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.5 Numbers</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Here we will try to find numbers on reviews and replace them with another string `numero` (that means `number`, in english). We could just replace the numbers with whitespace but maybe this would generated some information loss. Let's see what we've got:","metadata":{}},{"cell_type":"code","source":"def re_numbers(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('[0-9]+', ' numero ', r) for r in text_list]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.83308Z","end_time":"2020-07-25T16:22:12.838281Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.071050Z","iopub.status.idle":"2021-11-24T18:10:47.071882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying RegEx\nreviews_numbers = re_numbers(reviews_money)\ndf_comments['re_numbers'] = reviews_numbers\n\n# Verifying results\nprint_step_result(reviews_money, reviews_numbers, idx_list=[68])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:12.841685Z","end_time":"2020-07-25T16:22:13.103512Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.073349Z","iopub.status.idle":"2021-11-24T18:10:47.073879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.6\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.6 Negation</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"This session was thought and discussed in a special way. The problem statement is that when we remove the stopwords, probabily we would loose the meaning of some phrases about removing the negation words like `não` (not), for example. So, because of this, maybe is a good idea to replace some negation words with some common words indicating a negation meaning.","metadata":{}},{"cell_type":"code","source":"def re_negation(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', ' negação ', r) for r in text_list]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:13.107056Z","end_time":"2020-07-25T16:22:13.115033Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.075129Z","iopub.status.idle":"2021-11-24T18:10:47.075910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying RegEx\nreviews_negation = re_negation(reviews_numbers)\ndf_comments['re_negation'] = reviews_negation\n\n# Verifying results\nprint_step_result(reviews_numbers, reviews_negation, idx_list=[4783, 4627, 4856, 4904])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:13.118026Z","end_time":"2020-07-25T16:22:13.566164Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.077312Z","iopub.status.idle":"2021-11-24T18:10:47.077935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.7\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.7 Special Characters</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"The search for special characteres is a really special one because we see a lot of this pattern on online comments. Let's build an RegEx motor to find those ones.","metadata":{}},{"cell_type":"code","source":"def re_special_chars(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    return [re.sub('\\W', ' ', r) for r in text_list]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:13.568435Z","end_time":"2020-07-25T16:22:13.578829Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.079231Z","iopub.status.idle":"2021-11-24T18:10:47.079887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying RegEx\nreviews_special_chars = re_special_chars(reviews_negation)\ndf_comments['re_special_chars'] = reviews_special_chars\n\n# Verifying results\nprint_step_result(reviews_negation, reviews_special_chars, idx_list=[45, 135, 234])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:13.581811Z","end_time":"2020-07-25T16:22:14.114633Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.081161Z","iopub.status.idle":"2021-11-24T18:10:47.081748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.8\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.2.8 Additional Whitespaces</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After all the steps we have taken over here, it's important to clean our text eliminating unecessary whitespaces. Let's apply a RegEx for this and see what we've got.","metadata":{}},{"cell_type":"code","source":"def re_whitespaces(text_list):\n    \"\"\"\n    Args:\n    ----------\n    text_series: list object with text content to be prepared [type: list]\n    \"\"\"\n    \n    # Applying regex\n    white_spaces = [re.sub('\\s+', ' ', r) for r in text_list]\n    white_spaces_end = [re.sub('[ \\t]+$', '', r) for r in white_spaces]\n    return white_spaces_end","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:14.117932Z","end_time":"2020-07-25T16:22:14.131117Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.082940Z","iopub.status.idle":"2021-11-24T18:10:47.083625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying RegEx\nreviews_whitespaces = re_whitespaces(reviews_special_chars)\ndf_comments['re_whitespaces'] = reviews_whitespaces\n\n# Verifying results\nprint_step_result(reviews_special_chars, reviews_whitespaces, idx_list=[3, 4, -1])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:14.135107Z","end_time":"2020-07-25T16:22:14.919031Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.085044Z","iopub.status.idle":"2021-11-24T18:10:47.085770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.3 Stopwords</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, by now we have a text dataset without any pattern that we threated with RegEx and also without punctuations. In other words, we have a half-clean text with a rich transformation applied. \n\nSo, we are ready to apply some advanced text transformations like `stopwords` removal, `stemming` and the `TF-IDF` matrix process. Let's start with portuguese stopwords.","metadata":{}},{"cell_type":"code","source":"# Examples of some portuguese stopwords\npt_stopwords = stopwords.words('portuguese')\nprint(f'Total portuguese stopwords in the nltk.corpous module: {len(pt_stopwords)}')\npt_stopwords[:10]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:22:14.927595Z","end_time":"2020-07-25T16:22:14.951942Z"},"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.087178Z","iopub.status.idle":"2021-11-24T18:10:47.087797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining a function to remove the stopwords and to lower the comments\ndef stopwords_removal(text, cached_stopwords=stopwords.words('portuguese')):\n    \"\"\"\n    Args:\n    ----------\n    text: list object where the stopwords will be removed [type: list]\n    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]\n    \"\"\"\n    \n    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:42:58.650673Z","end_time":"2020-07-25T16:42:58.658651Z"},"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.089325Z","iopub.status.idle":"2021-11-24T18:10:47.089871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing stopwords and looking at some examples\nreviews_stopwords = [' '.join(stopwords_removal(review)) for review in reviews_whitespaces]\ndf_comments['stopwords_removed'] = reviews_stopwords\n\nprint_step_result(reviews_whitespaces, reviews_stopwords, idx_list=[0, 45, 500])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:48:21.9528Z","end_time":"2020-07-25T16:48:23.187108Z"},"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.091257Z","iopub.status.idle":"2021-11-24T18:10:47.092142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.4 Stemming</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Let's define a function to apply the stemming process on the comments. We will also give examples of the results.","metadata":{}},{"cell_type":"code","source":"# Defining a function to remove the stopwords and to lower the comments\ndef stemming_process(text, stemmer=RSLPStemmer()):\n    \"\"\"\n    Args:\n    ----------\n    text: list object where the stopwords will be removed [type: list]\n    stemmer: type of stemmer to be applied [type: class, default: RSLPStemmer()]\n    \"\"\"\n    \n    return [stemmer.stem(c) for c in text.split()]","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:39:23.350571Z","end_time":"2020-07-25T16:39:23.359447Z"},"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.094264Z","iopub.status.idle":"2021-11-24T18:10:47.094993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying stemming and looking at some examples\nreviews_stemmer = [' '.join(stemming_process(review)) for review in reviews_stopwords]\ndf_comments['stemming'] = reviews_stemmer\n\nprint_step_result(reviews_stopwords, reviews_stemmer, idx_list=[0, 45, -1])","metadata":{"ExecuteTime":{"start_time":"2020-07-25T16:55:55.156272Z","end_time":"2020-07-25T16:56:13.817244Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.096746Z","iopub.status.idle":"2021-11-24T18:10:47.097347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.5\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.5 Feature Extraction</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, once we have passe through RegEx, stopwords removal and stemming application, to give more meaning for the text we are analysing, we can use approachs like _Bag of Words_, _TF-IDF_ and _Word2Vec_. For make our analysis easier, let's define a function that receives a text and a `vectorizer` object and applies the feature extraction on the respective text.","metadata":{}},{"cell_type":"code","source":"def extract_features_from_corpus(corpus, vectorizer, df=False):\n    \"\"\"\n    Args\n    ------------\n    text: text to be transformed into a document-term matrix [type: string]\n    vectorizer: engine to be used in the transformation [type: object]\n    \"\"\"\n    \n    # Extracting features\n    corpus_features = vectorizer.fit_transform(corpus).toarray()\n    features_names = vectorizer.get_feature_names()\n    \n    # Transforming into a dataframe to give interpetability to the process\n    df_corpus_features = None\n    if df:\n        df_corpus_features = pd.DataFrame(corpus_features, columns=features_names)\n    \n    return corpus_features, df_corpus_features","metadata":{"ExecuteTime":{"start_time":"2020-07-25T17:37:31.439433Z","end_time":"2020-07-25T17:37:31.44839Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.098674Z","iopub.status.idle":"2021-11-24T18:10:47.099455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.5.1\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.5.1 CountVectorizer</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"On the _Bag of Words_ approach, we create a dicitonary vocabulary with all the unique words and, for each word in each comment/text string, we index the words into a vector that represents the occurence (1) or not (0) of each word. This is a way for transforming a text into a frequency vector considering a literal bag of words (dictionary vocabulary).","metadata":{}},{"cell_type":"code","source":"# Creating an object for the CountVectorizer class\ncount_vectorizer = CountVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n\n# Extracting features for the corpus\ncountv_features, df_countv_features = extract_features_from_corpus(reviews_stemmer, count_vectorizer, df=True)\nprint(f'Shape of countv_features matrix: {countv_features.shape}\\n')\nprint(f'Example of DataFrame of corpus features:')\ndf_countv_features.head()","metadata":{"ExecuteTime":{"start_time":"2020-07-25T17:41:26.355416Z","end_time":"2020-07-25T17:41:27.142818Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.100786Z","iopub.status.idle":"2021-11-24T18:10:47.101587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.5.2\"></a>\n<font color=\"dimgrey\" size=+1.5><b>4.5.2 TF-IDF</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"With the _Bag of Words_ approach, each words has the same weight, wich maybe can't be true all the time, mainly for those words with a really low frequency on the corpus. So, the _TF-IDF (Term Frequency and Inverse Document Frequency)_ approach can be used with the scikit-learn library following the formulas:\n\n$$TF=\\frac{\\text{Frequency of a word in the document}}{\\text{Total words in the document}}$$","metadata":{}},{"cell_type":"markdown","source":"$$IDF = \\log\\left({\\frac{\\text{Total number of docs}}{\\text{Number of docs containing the words}}}\\right)$$","metadata":{}},{"cell_type":"code","source":"# Creating an object for the CountVectorizer class\ntfidf_vectorizer = TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n\n# Extracting features for the corpus\ntfidf_features, df_tfidf_features = extract_features_from_corpus(reviews_stemmer, tfidf_vectorizer, df=True)\nprint(f'Shape of tfidf_features matrix: {tfidf_features.shape}\\n')\nprint(f'Example of DataFrame of corpus features:')\ndf_tfidf_features.head()","metadata":{"ExecuteTime":{"start_time":"2020-07-25T17:41:47.638303Z","end_time":"2020-07-25T17:41:48.314455Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.103264Z","iopub.status.idle":"2021-11-24T18:10:47.104007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The attributes used during the instancing of each vectorizer objects can be explained as:\n\n- `max_features=300`: indicates that the matrix will be created using the 300 most common words from the corpus\n- `max_df=0.8`: indicates that we will use only words with at least 80% frequency in the corpus\n- `min_df=7`: indicates that we will use only words that occurs in at least 7 text strings in the corpus","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.6\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.6 Labelling Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"For training a sentimental analysis model, we must need the label to apply in a supervisioned Machine Learning approach. The dataset we doesn't have a clearly label saying wich comment is positive or negative. For doing that, probably the best approach is to look at individual comments and label it handly with 1 (positive comment) and 0 (negative comment) but, thinking in a fast implementation, we will use the `review_score` column to label our data into those two classes. Let's take a look.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 5))\nsingle_countplot(x='score', df=df_comments, ax=ax)","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.639Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.105804Z","iopub.status.idle":"2021-11-24T18:10:47.106361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this approach, let's consider that every comment with scores 1, 2 and 3 are negative comments. In the other hand, comments with score 4 and 5 will be considered as positive. Again, probably this is not the best way to train a sentimental analysis model, but for fastness, we will do this assumption and see if we can extract value from it.","metadata":{"ExecuteTime":{"start_time":"2020-07-26T19:24:56.856757Z","end_time":"2020-07-26T19:24:56.865663Z"},"trusted":true}},{"cell_type":"code","source":"# Labelling data\nscore_map = {\n    1: 'negative',\n    2: 'negative',\n    3: 'positive',\n    4: 'positive',\n    5: 'positive'\n}\ndf_comments['sentiment_label'] = df_comments['score'].map(score_map)\n\n# Verifying results\nfig, ax = plt.subplots(figsize=(7, 7))\ndonut_plot(df_comments.query('sentiment_label in (\"positive\", \"negative\")'), 'sentiment_label', \n           label_names=df_comments.query('sentiment_label in (\"positive\", \"negative\")')['sentiment_label'].value_counts().index,\n           ax=ax, colors=['darkslateblue', 'crimson'])","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.679Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.107619Z","iopub.status.idle":"2021-11-24T18:10:47.108243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n* _What's the main n-grams presentes in corpus on positive and negative classes?_\n___","metadata":{}},{"cell_type":"markdown","source":"I want to give credits to Raenish David notebook [Tweet Sentiment - Insight EDA](https://www.kaggle.com/raenish/tweet-sentiment-insight-eda) that inspired this analysis.","metadata":{}},{"cell_type":"code","source":"def ngrams_count(corpus, ngram_range, n=-1, cached_stopwords=stopwords.words('portuguese')):\n    \"\"\"\n    Args\n    ----------\n    corpus: text to be analysed [type: pd.DataFrame]\n    ngram_range: type of n gram to be used on analysis [type: tuple]\n    n: top limit of ngrams to be shown [type: int, default: -1]\n    \"\"\"\n    \n    # Using CountVectorizer to build a bag of words using the given corpus\n    vectorizer = CountVectorizer(stop_words=cached_stopwords, ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vectorizer.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    total_list = words_freq[:n]\n    \n    # Returning a DataFrame with the ngrams count\n    count_df = pd.DataFrame(total_list, columns=['ngram', 'count'])\n    return count_df","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.703Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.109702Z","iopub.status.idle":"2021-11-24T18:10:47.110472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the corpus into positive and negative comments\npositive_comments = df_comments.query('sentiment_label == \"positive\"')['stemming']\nnegative_comments = df_comments.query('sentiment_label == \"negative\"')['stemming']\n\n# Extracting the top 10 unigrams by sentiment\nunigrams_pos = ngrams_count(positive_comments, (1, 1), 10)\nunigrams_neg = ngrams_count(negative_comments, (1, 1), 10)\n\n# Extracting the top 10 unigrams by sentiment\nbigrams_pos = ngrams_count(positive_comments, (2, 2), 10)\nbigrams_neg = ngrams_count(negative_comments, (2, 2), 10)\n\n# Extracting the top 10 unigrams by sentiment\ntrigrams_pos = ngrams_count(positive_comments, (3, 3), 10)\ntrigrams_neg = ngrams_count(negative_comments, (3, 3), 10)","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.726Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.111839Z","iopub.status.idle":"2021-11-24T18:10:47.112485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joining everything in a python dictionary to make the plots easier\nngram_dict_plot = {\n    'Top Unigrams on Positive Comments': unigrams_pos,\n    'Top Unigrams on Negative Comments': unigrams_neg,\n    'Top Bigrams on Positive Comments': bigrams_pos,\n    'Top Bigrams on Negative Comments': bigrams_neg,\n    'Top Trigrams on Positive Comments': trigrams_pos,\n    'Top Trigrams on Negative Comments': trigrams_neg,\n}\n\n# Plotting the ngrams analysis\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 18))\ni, j = 0, 0\ncolors = ['Blues_d', 'Reds_d']\nfor title, ngram_data in ngram_dict_plot.items():\n    ax = axs[i, j]\n    sns.barplot(x='count', y='ngram', data=ngram_data, ax=ax, palette=colors[j])\n    \n    # Customizing plots\n    format_spines(ax, right_border=False)\n    ax.set_title(title, size=14)\n    ax.set_ylabel('')\n    ax.set_xlabel('')\n    \n    # Incrementing the index\n    j += 1\n    if j == 2:\n        j = 0\n        i += 1\nplt.tight_layout()\nplt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.745Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.113849Z","iopub.status.idle":"2021-11-24T18:10:47.114467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The barcharts above are interesting and they really reflect the sentiment based on ngrams. We can clearly see negative words on bigrams and trigrams by the right side of the figure (the most frequent trigram `neg receb produt` in english maybe means something like `didn't receive the product` for example).\n\nThe positive bigrams and trigrams at the blue left side of the figure really consists of positive words (the most frequent trigram `entreg ant praz` means something like `delivery before time` in english). Also!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.7\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.7 Pipeline</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"So, after detailing all the steps considered on this text prep pipeline, let's build a code to apply a complete pipeline automatically to handle it. This is a important step on the project because with this we can receive a text input and apply all changes on it to make it ready for training or predicting the sentiment label.","metadata":{}},{"cell_type":"code","source":"# Class for regular expressions application\nclass ApplyRegex(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, regex_transformers):\n        self.regex_transformers = regex_transformers\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Applying all regex functions in the regex_transformers dictionary\n        for regex_name, regex_function in self.regex_transformers.items():\n            X = regex_function(X)\n            \n        return X\n\n# Class for stopwords removal from the corpus\nclass StopWordsRemoval(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, text_stopwords):\n        self.text_stopwords = text_stopwords\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return [' '.join(stopwords_removal(comment, self.text_stopwords)) for comment in X]\n\n# Class for apply the stemming process\nclass StemmingProcess(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, stemmer):\n        self.stemmer = stemmer\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return [' '.join(stemming_process(comment, self.stemmer)) for comment in X]\n    \n# Class for extracting features from corpus\nclass TextFeatureExtraction(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, vectorizer):\n        self.vectorizer = vectorizer\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return self.vectorizer.fit_transform(X).toarray()","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.763Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.116048Z","iopub.status.idle":"2021-11-24T18:10:47.116690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining regex transformers to be applied\nregex_transformers = {\n    'break_line': re_breakline,\n    'hiperlinks': re_hiperlinks,\n    'dates': re_dates,\n    'money': re_money,\n    'numbers': re_numbers,\n    'negation': re_negation,\n    'special_chars': re_special_chars,\n    'whitespaces': re_whitespaces\n}\n\n# Defining the vectorizer to extract features from text\nvectorizer = TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=pt_stopwords)\n\n# Building the Pipeline\ntext_pipeline = Pipeline([\n    ('regex', ApplyRegex(regex_transformers)),\n    ('stopwords', StopWordsRemoval(stopwords.words('portuguese'))),\n    ('stemming', StemmingProcess(RSLPStemmer())),\n    ('text_features', TextFeatureExtraction(vectorizer))\n])","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.775Z"},"code_folding":[],"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.117548Z","iopub.status.idle":"2021-11-24T18:10:47.117962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just to remember, in the pipeline above we chose the TF-IDF approach to extract features from text using the same parameters we used on the examples (`max_features=300`, `min_df=7`, `max_df=0.8`). It means that every text string in our corpus will be 300 \"text features\" respecting the criteria defined by the min_df and max_df parameters. Let's apply it.","metadata":{}},{"cell_type":"code","source":"# Defining X and y \nidx_reviews = olist_order_reviews['review_comment_message'].dropna().index\nscore = olist_order_reviews['review_score'][idx_reviews].map(score_map)\n\n# Splitting into train and test sets\nX = list(olist_order_reviews['review_comment_message'][idx_reviews].values)\ny = score.apply(lambda x: 1 if x == 'positive' else 0).values\n\n# Applying the pipeline and splitting the data\nX_processed = text_pipeline.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=.20, random_state=42)\n\n# Verifying results\nprint(f'Length of X_train_processed: {len(X_train)} - Length of one element: {len(X_train[0])}')\nprint(f'Length of X_test_processed: {len(X_test)} - Length of one element: {len(X_test[0])}')","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.861Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.118965Z","iopub.status.idle":"2021-11-24T18:10:47.119521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>5. Sentiment Classification</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, we went trought a lot of steps together and this is the final one! After all the text preparation we've done, it's now time to put it together into a classification model to train an algorithm that understands wherever a text string has a `positive` or a `negative` feeling based on the features we extracted from the corpus.\n\nSo, let's use a homemade class for make the training and analysis easier.","metadata":{"ExecuteTime":{"start_time":"2020-07-26T21:20:47.440077Z","end_time":"2020-07-26T21:20:47.44606Z"}}},{"cell_type":"code","source":"# Logistic Regression hyperparameters\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Setting up the classifiers\nset_classifiers = {\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'params': logreg_param_grid\n    },\n    'Naive Bayes': {\n        'model': GaussianNB(),\n        'params': {}\n    }\n}","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.871Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.120797Z","iopub.status.idle":"2021-11-24T18:10:47.121174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n* _Training selected Machine Learning models_\n___","metadata":{}},{"cell_type":"code","source":"# Creating an object and training the classifiers\nclf_tool = BinaryClassifiersAnalysis()\nclf_tool.fit(set_classifiers, X_train, y_train, random_search=True, scoring='accuracy')","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.886Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.122113Z","iopub.status.idle":"2021-11-24T18:10:47.122448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n* _Evaluating models_\n___","metadata":{}},{"cell_type":"code","source":"# Evaluating metrics\ndf_performances = clf_tool.evaluate_performance(X_train, y_train, X_test, y_test, cv=5)\ndf_performances.reset_index(drop=True).style.background_gradient(cmap='Blues')","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.915Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.123496Z","iopub.status.idle":"2021-11-24T18:10:47.123868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n* _Confusion Matrix_\n___","metadata":{}},{"cell_type":"code","source":"clf_tool.plot_confusion_matrix(classes=['Negative', 'Positive'])","metadata":{"ExecuteTime":{"start_time":"2020-07-27T01:24:00.989Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.124997Z","iopub.status.idle":"2021-11-24T18:10:47.125523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>6. Final Implementation</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Finally we can build up our final step to delivery a Sentiment Analysis model! We have a full prep pipeline, a machine learning model (to choose) and now the only thing we can do to improve it is to build a connected solution that can receive a input text string (say an e-commerce comment) and return its sentiment. Let's try!","metadata":{}},{"cell_type":"code","source":"# Defining a function to plot the sentiment of a given phrase\ndef sentiment_analysis(text, pipeline, vectorizer, model):\n    \"\"\"\n    Args\n    -----------\n    text: text string / phrase / review comment to be analysed [type: string]\n    pipeline: text prep pipeline built for preparing the corpus [type: sklearn.Pipeline]\n    model: classification model trained to recognize positive and negative sentiment [type: model]\n    \"\"\"\n    \n    # Applying the pipeline\n    if type(text) is not list:\n        text = [text]\n    text_prep = pipeline.fit_transform(text)\n    matrix = vectorizer.transform(text_prep)\n    \n    # Predicting sentiment\n    pred = model.predict(matrix)\n    proba = model.predict_proba(matrix)\n    \n    # Plotting the sentiment and its score\n    fig, ax = plt.subplots(figsize=(5, 3))\n    if pred[0] == 1:\n        text = 'Positive'\n        class_proba = 100 * round(proba[0][1], 2)\n        color = 'seagreen'\n    else:\n        text = 'Negative'\n        class_proba = 100 * round(proba[0][0], 2)\n        color = 'crimson'\n    ax.text(0.5, 0.5, text, fontsize=50, ha='center', color=color)\n    ax.text(0.5, 0.20, str(class_proba) + '%', fontsize=14, ha='center')\n    ax.axis('off')\n    ax.set_title('Sentiment Analysis', fontsize=14)\n    plt.show()","metadata":{"ExecuteTime":{"start_time":"2020-07-27T20:53:35.445353Z","end_time":"2020-07-27T20:53:35.458411Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.126456Z","iopub.status.idle":"2021-11-24T18:10:47.126777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining transformers for preparing the text input\nmodel = clf_tool.classifiers_info['LogisticRegression']['estimator']\nprod_pipeline = Pipeline([\n    ('regex', ApplyRegex(regex_transformers)),\n    ('stopwords', StopWordsRemoval(stopwords.words('portuguese'))),\n    ('stemming', StemmingProcess(RSLPStemmer()))\n])\nvectorizer = text_pipeline.named_steps['text_features'].vectorizer","metadata":{"ExecuteTime":{"start_time":"2020-07-27T20:53:47.20332Z","end_time":"2020-07-27T20:53:47.221907Z"},"execution":{"iopub.status.busy":"2021-11-24T18:10:47.127699Z","iopub.status.idle":"2021-11-24T18:10:47.128182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try to write a phrase to feed our `sentiment_analysis` function. In production, we can adapt it to serve any application. So, let's simulate an online review that says something like `Very bad product! I don't buy on this store anymore, the delivery was late and it cost so much money`.\n\nWhat's the sentiment of this given phrase?","metadata":{}},{"cell_type":"code","source":"comment = 'Péssimo produto! Não compro nessa loja, a entrega atrasou e custou muito dinheiro!'\nsentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)","metadata":{"ExecuteTime":{"start_time":"2020-07-27T20:57:40.379757Z","end_time":"2020-07-27T20:57:40.591289Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.129283Z","iopub.status.idle":"2021-11-24T18:10:47.129890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excelent! Our model returned exactly we expected. Now, let' simulate a comment that says something like `I love it and it really fulfilled the expectations. I bought for a cheap value. Wonderful`.","metadata":{}},{"cell_type":"code","source":"comment = 'Adorei e realmente cumpriu as expectativas. Comprei por um valor barato. Maravilhoso'\nsentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)","metadata":{"ExecuteTime":{"start_time":"2020-07-27T21:05:41.282379Z","end_time":"2020-07-27T21:05:41.475841Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.130973Z","iopub.status.idle":"2021-11-24T18:10:47.131488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At least, let's try to simulate a very neutral comment. Something like `I don't know if a liked this product. The cost was cheap but it was defectuous. If you're lucky, it worths`","metadata":{}},{"cell_type":"code","source":"comment = 'Não sei gostei do produto. O custo foi barato mas veio com defeito. Se der sorte, vale a pena'\nsentiment_analysis(comment, pipeline=prod_pipeline, vectorizer=vectorizer, model=model)","metadata":{"ExecuteTime":{"start_time":"2020-07-27T21:10:49.341217Z","end_time":"2020-07-27T21:10:49.653153Z"},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.132818Z","iopub.status.idle":"2021-11-24T18:10:47.133274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the end, let's plot a WordCloud for positive and negative words on our dataset.","metadata":{}},{"cell_type":"code","source":"# Reading and preparing a mask for serving as wordcloud background\nlike_mask = np.array(Image.open(\"../input/imgicons/like.png\"))\nbomb_mask = np.array(Image.open(\"../input/imgicons/bomb3.png\"))\n#angry_mask = angry_mask[:, :, -1]\n\n# Transforming like mask\ntransf_like_mask = np.ndarray((like_mask.shape[0], like_mask.shape[1]), np.int32)\nfor i in range(len(like_mask)):\n    transf_like_mask[i] = [255 if px == 0 else 0 for px in like_mask[i]]\n\n# Transforming angry mask\ntransf_bomb_mask = np.ndarray((bomb_mask.shape[0], bomb_mask.shape[1]), np.int32)\nfor i in range(len(bomb_mask)):\n    transf_bomb_mask[i] = [255 if px == 0 else 0 for px in bomb_mask[i]]\n    \n# Generating words\npos_comments = list(df_comments.query('sentiment_label == \"positive\"')['stopwords_removed'].values)\npositive_words = ' '.join(pos_comments).split(' ')\nneg_comments = list(df_comments.query('sentiment_label == \"negative\"')['stopwords_removed'].values)\nnegative_words = ' '.join(neg_comments).split(' ')\n\n# Using Counter for creating a dictionary counting\npositive_dict = Counter(positive_words)\nnegative_dict = Counter(negative_words)\n\n# Generating wordclouds for both positive and negative comments\npositive_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42, mask=transf_like_mask,\n                      colormap='Blues', background_color='white', max_words=50).generate_from_frequencies(positive_dict)\nnegative_wc = WordCloud(width=1280, height=720, collocations=False, random_state=42, mask=transf_bomb_mask,\n                      colormap='Reds', background_color='white', max_words=50).generate_from_frequencies(negative_dict)\n\n# Visualizing the WC created and the total for each cuisine\nfig, axs = plt.subplots(1, 2, figsize=(20, 20))\nax1 = axs[0]\nax2 = axs[1]\n\nax1.imshow(positive_wc)\nax1.axis('off')\nax1.set_title('WordCloud for Positive Words in Reviews', size=18, pad=20)\n\nax2.imshow(negative_wc)\nax2.axis('off')\nax2.set_title('WordCloud for Negative Words in Reviews', size=18, pad=20)\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T18:10:47.134347Z","iopub.status.idle":"2021-11-24T18:10:47.134776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>7. Conclusion</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, it was a long journey and I hope you all had experienced a really explained and useful notebook for a Sentimental Analysis task. We could detail step by step on how to input a dataset with comment reviews of online purchasing operations and extract the sentiment of people that left their reviews. We now are able to expand this to a high level application that automatic detects the sentiment of a given text or phrase.\n\n","metadata":{}},{"cell_type":"markdown","source":"[](http://)<a id=\"8\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>8. Complete Script</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Now, by the end of our detailed implementation, I want to share with you how this project could be implemented as complete structure project in production. The script above is exactly the same the one I built in my machine to prepare myself for a real problem.","metadata":{}},{"cell_type":"markdown","source":"___\n* _Project Transformers_\n___","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThis python script will allocate all the custom transformers that are specific for the project task.\nThe idea is to encapsulate the classes and functions used on pipelines to make codes cleaner.\n\"\"\"\n\n# Importing libraries\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\n\"\"\"\n-----------------------------------\n----- 1. CUSTOM TRANSFORMERS ------\n           1.1 Classes\n-----------------------------------\n\"\"\"\n\n\nclass ColumnMapping(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class applies the map() function into a DataFrame for transforming a columns given a mapping dictionary\n\n    Parameters\n    ----------\n    :param old_col_name: name of the columns where mapping will be applied [type: string]\n    :param mapping_dict: python dictionary with key/value mapping [type: dict]\n    :param new_col_name: name of the new column resulted by mapping [type: string, default: 'target]\n    :param drop: flag that guides the dropping of the old_target_name column [type: bool, default: True]\n\n    Returns\n    -------\n    :return X: pandas DataFrame object after mapping application [type: pd.DataFrame]\n\n    Application\n    -----------\n    # Transforming a DataFrame column given a mapping dictionary\n    mapper = ColumnMapping(old_col_name='col_1', mapping_dict=dictionary, new_col_name='col_2', drop=True)\n    df_mapped = mapper.fit_transform(df)\n    \"\"\"\n\n    def __init__(self, old_col_name, mapping_dict, new_col_name='target', drop=True):\n        self.old_col_name = old_col_name\n        self.mapping_dict = mapping_dict\n        self.new_col_name = new_col_name\n        self.drop = drop\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        # Applying mapping\n        X[self.new_col_name] = X[self.old_col_name].map(self.mapping_dict)\n\n        # Dropping the old columns (if applicable)\n        if self.drop:\n            X.drop(self.old_col_name, axis=1, inplace=True)\n\n        return X","metadata":{"execution":{"iopub.status.busy":"2021-11-24T18:10:47.135881Z","iopub.status.idle":"2021-11-24T18:10:47.136460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___\n* _Complete Python Script for Training a Sentiment Classififier_\n___","metadata":{}},{"cell_type":"markdown","source":"The code below is end-to-end solution for creating a sentiment classification model using the olist e-commerce reviews dataset. As I said before, this script was built on my personal machine, so I commented the lines of code that saves something on the fileserver.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThis python script are responsible for reading, preparing and training a sentiment classification model from\ne-commerce reviews taken from brazilian web-sites\n\n* Metadata can be find at: https://www.kaggle.com/olistbr/brazilian-ecommerce\n* Reference notebook: ../notebooks/EDA_BrazilianECommerce.ipynb\n\n--- SUMMARY ---\n\n1. Project Variables\n2. Reading Data\n3. Prep Pipelines\n    3.1 Initial Preparation\n    3.2 Text Transformers\n4. Modeling\n    4.1 Model Training\n    4.2 Evaluating Metrics\n    4.3 Complete Solution\n    4.4 Final Model Performance\n    4.5 Saving pkl Files\n\n---------------------------------------------------------------\nWritten by Thiago Panini - Latest version: September 24th 2020\n---------------------------------------------------------------\n\"\"\"\n\n\n# Importing libs\nimport os\nimport numpy as np\nimport pandas as pd\n#from dev.training.project_transformers import ColumnMapping\nfrom custom_transformers import import_data, DropNullData, DropDuplicates\nfrom text_utils import re_breakline, re_dates, re_hiperlinks, re_money, re_negation, re_numbers, \\\n    re_special_chars, re_whitespaces, ApplyRegex, StemmingProcess, StopWordsRemoval\nfrom nltk.corpus import stopwords\nfrom nltk.stem import RSLPStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom joblib import dump\nfrom sklearn.linear_model import LogisticRegression\nfrom ml_utils import BinaryClassifiersAnalysis, cross_val_performance\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve\n\n\n\"\"\"\n-----------------------------------\n------ 1. PROJECT VARIABLES -------\n-----------------------------------\n\"\"\"\n\n# Variables for address paths\nDATA_PATH = '../input/brazilian-ecommerce'\nPIPELINES_PATH = '../../pipelines' # Take a look at your project structure\nMODELS_PATH = '../../models' # Take a look at your project structure\n\n# Variables for reading the data\nFILENAME = 'olist_order_reviews_dataset.csv'\nCOLS_READ = ['review_comment_message', 'review_score']\nCORPUS_COL = 'review_comment_message'\nTARGET_COL = 'target'\n\n# Defining stopwords\nPT_STOPWORDS = stopwords.words('portuguese')\n\n# Variables for saving data\nMETRICS_FILEPATH = 'metrics/model_performance.csv' # Take a look at your project structure\n\n# Variables for retrieving model\nMODEL_KEY = 'LogisticRegression'\n\n\n\"\"\"\n-----------------------------------\n-------- 2. READING DATA ----------\n-----------------------------------\n\"\"\"\n\n# Reading the data with text corpus and score\ndf = import_data(os.path.join(DATA_PATH, FILENAME), usecols=COLS_READ)\n\n\n\"\"\"\n-----------------------------------\n------- 3. PREP PIPELINES ---------\n    3.1 Initial Preparation\n-----------------------------------\n\"\"\"\n\n# Creating a dictionary for mapping the target column based on review score\nscore_map = {\n    1: 0,\n    2: 0,\n    3: 0,\n    4: 1,\n    5: 1\n}\n\n# Creating a pipeline for the initial prep on the data\ninitial_prep_pipeline = Pipeline([\n    ('mapper', ColumnMapping(old_col_name='review_score', mapping_dict=score_map, new_col_name=TARGET_COL)),\n    ('null_dropper', DropNullData()),\n    ('dup_dropper', DropDuplicates())\n])\n\n# Applying initial prep pipeline\ndf_prep = initial_prep_pipeline.fit_transform(df)\n\n\n\"\"\"\n-----------------------------------\n------- 3. PREP PIPELINES ---------\n      3.2 Text Transformers\n-----------------------------------\n\"\"\"\n\n# Defining regex transformers to be applied\nregex_transformers = {\n    'break_line': re_breakline,\n    'hiperlinks': re_hiperlinks,\n    'dates': re_dates,\n    'money': re_money,\n    'numbers': re_numbers,\n    'negation': re_negation,\n    'special_chars': re_special_chars,\n    'whitespaces': re_whitespaces\n}\n\n# Building a text prep pipeline\ntext_prep_pipeline = Pipeline([\n    ('regex', ApplyRegex(regex_transformers)),\n    ('stopwords', StopWordsRemoval(PT_STOPWORDS)),\n    ('stemming', StemmingProcess(RSLPStemmer())),\n    ('vectorizer', TfidfVectorizer(max_features=300, min_df=7, max_df=0.8, stop_words=PT_STOPWORDS))\n])\n\n# Applying the pipeline\nX = df_prep[CORPUS_COL].tolist()\ny = df_prep[TARGET_COL]\nX_prep = text_prep_pipeline.fit_transform(X)\n\n# Splitting the data into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(X_prep, y, test_size=.20, random_state=42)\n\n# Saving states before prep pipeline\n\"\"\"df_prep[CORPUS_COL].to_csv(os.path.join(DATA_PATH, 'X_data.csv'), index=False)\ndf_prep[TARGET_COL].to_csv(os.path.join(DATA_PATH, 'y_data.csv'), index=False)\"\"\"\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n       4.1 Model Training\n-----------------------------------\n\"\"\"\n\n# Specifing a Logistic Regression model for sentiment classification\nlogreg_param_grid = {\n    'C': np.linspace(0.1, 10, 20),\n    'penalty': ['l1', 'l2'],\n    'class_weight': ['balanced', None],\n    'random_state': [42],\n    'solver': ['liblinear']\n}\n\n# Setting up the classifiers\nset_classifiers = {\n    'LogisticRegression': {\n        'model': LogisticRegression(),\n        'params': logreg_param_grid\n    }\n}\n\n# Creating an object and training the classifiers\ntrainer = BinaryClassifiersAnalysis()\ntrainer.fit(set_classifiers, X_train, y_train, random_search=True, scoring='accuracy')\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n    4.2 Evaluating Metrics\n-----------------------------------\n\"\"\"\n\n# Evaluating metrics\nperformance = trainer.evaluate_performance(X_train, y_train, X_test, y_test, cv=5, save=False,\n                                           performances_filepath=METRICS_FILEPATH) # In your project env, save=True and overwrite=True may be useful\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n    4.3. Complete Solution\n-----------------------------------\n\"\"\"\n\n# Returning the model to be saved\nmodel = trainer.classifiers_info[MODEL_KEY]['estimator']\n\n# Creating a complete pipeline for prep and predict\ne2e_pipeline = Pipeline([\n    ('text_prep', text_prep_pipeline),\n    ('model', model)\n])\n\n# Defining a param grid for searching best pipelines options (reduced options for making the search faster)\n\"\"\"param_grid = [{\n    'text_prep__vectorizer__max_features': np.arange(500, 851, 50),\n    'text_prep__vectorizer__min_df': [7, 9, 12, 15, 30],\n    'text_prep__vectorizer__max_df': [.4, .5, .6, .7]\n}]\"\"\"\n\nparam_grid = [{\n    'text_prep__vectorizer__max_features': np.arange(500, 501, 50),\n    'text_prep__vectorizer__min_df': [7],\n    'text_prep__vectorizer__max_df': [.4]\n}]\n\n# Searching for best options\ngrid_search_prep = GridSearchCV(e2e_pipeline, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\ngrid_search_prep.fit(X, y)\nprint('Best params after a complete search:')\nprint(grid_search_prep.best_params_)\n\n# Returning the best options\nvectorizer_max_features = grid_search_prep.best_params_['text_prep__vectorizer__max_features']\nvectorizer_min_df = grid_search_prep.best_params_['text_prep__vectorizer__min_df']\nvectorizer_max_df = grid_search_prep.best_params_['text_prep__vectorizer__max_df']\n\n# Updating the e2e pipeline with the best options found on search\ne2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].max_features = vectorizer_max_features\ne2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].min_df = vectorizer_min_df\ne2e_pipeline.named_steps['text_prep'].named_steps['vectorizer'].max_df = vectorizer_max_df\n\n# Fitting the model again\ne2e_pipeline.fit(X, y)\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n    4.4 Final Model Performance\n-----------------------------------\n\"\"\"\n\n# Retrieving performance for te final model after hyperparam updating\nfinal_model = e2e_pipeline.named_steps['model']\nfinal_performance = cross_val_performance(final_model, X_prep, y, cv=5)\nfinal_performance = final_performance.append(performance)\nprint(final_performance)\n#final_performance.to_csv(METRICS_FILEPATH, index=False)\n\n\n\"\"\"\n-----------------------------------\n--------- 4. MODELING  -----------\n      4.5 Saving pkl files\n-----------------------------------\n\"\"\"\n\n\"\"\"# Creating folders for saving pkl files (if not exists)\nif not os.path.exists('../../models'):\n    os.makedirs('../../models')\nif not os.path.exists('../../pipelines'):\n    os.makedirs('../../pipelines')\n\n# Saving pkl files\ndump(initial_prep_pipeline, os.path.join(PIPELINES_PATH, 'initial_prep_pipeline.pkl'))\ndump(text_prep_pipeline, os.path.join(PIPELINES_PATH, 'text_prep_pipeline.pkl'))\ndump(e2e_pipeline, os.path.join(PIPELINES_PATH, 'e2e_pipeline.pkl'))\ndump(final_model, os.path.join(MODELS_PATH, 'sentiment_clf_model.pkl'))\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-24T18:10:47.137875Z","iopub.status.idle":"2021-11-24T18:10:47.138332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**References:**\n\nhttps://www.kaggle.com/raenish/cheatsheet-text-helper-functions created by Raenish David\n\nhttps://www.kaggle.com/andresionek/geospatial-analysis-of-brazilian-e-commerce created by Andre Sionek","metadata":{}},{"cell_type":"markdown","source":"<font size=\"+1\" color=\"black\"><b>Please visit my other kernels by clicking on the buttons</b></font><br>\n\n<a href=\"https://www.kaggle.com/thiagopanini/predicting-restaurant-s-rate-in-bengaluru\" class=\"btn btn-primary\" style=\"color:white;\">Bengaluru's Restaurants</a>\n<a href=\"https://www.kaggle.com/thiagopanini/predicting-credit-risk-eda-viz-pipeline#Training-and-Evaluating-a-Model\" class=\"btn btn-primary\" style=\"color:white;\">Credit Risk Detection</a>\n<a href=\"https://www.kaggle.com/thiagopanini/credit-fraud-how-to-choose-the-best-classifier\" class=\"btn btn-primary\" style=\"color:white;\">Credit Card Fraud Detection</a>\n<a href=\"https://www.kaggle.com/thiagopanini/global-terrorism-eda-nlp\" class=\"btn btn-primary\" style=\"color:white;\">Global Terrorism</a>","metadata":{}}]}